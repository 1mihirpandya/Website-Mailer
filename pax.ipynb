{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook For ReadMyMind, A CS 125 @ Illinois MP7 Project by Isaac Park and Mihir Pandya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Avg. Likes</th>\n",
       "      <th>Avg. Retweets</th>\n",
       "      <th>Avg. Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>great</td>\n",
       "      <td>17</td>\n",
       "      <td>74423</td>\n",
       "      <td>15508</td>\n",
       "      <td>0.632480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>america</td>\n",
       "      <td>15</td>\n",
       "      <td>77764</td>\n",
       "      <td>19465</td>\n",
       "      <td>0.631233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>12</td>\n",
       "      <td>88798</td>\n",
       "      <td>23246</td>\n",
       "      <td>0.609117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pensacola</td>\n",
       "      <td>9</td>\n",
       "      <td>61735</td>\n",
       "      <td>13868</td>\n",
       "      <td>0.372297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>cuts</td>\n",
       "      <td>9</td>\n",
       "      <td>72724</td>\n",
       "      <td>16913</td>\n",
       "      <td>0.517917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tax</td>\n",
       "      <td>9</td>\n",
       "      <td>61127</td>\n",
       "      <td>15095</td>\n",
       "      <td>0.470904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>cnn</td>\n",
       "      <td>7</td>\n",
       "      <td>112497</td>\n",
       "      <td>29027</td>\n",
       "      <td>0.537169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>news</td>\n",
       "      <td>7</td>\n",
       "      <td>97927</td>\n",
       "      <td>25942</td>\n",
       "      <td>0.581112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>market</td>\n",
       "      <td>7</td>\n",
       "      <td>96695</td>\n",
       "      <td>21615</td>\n",
       "      <td>0.517571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bill</td>\n",
       "      <td>6</td>\n",
       "      <td>82313</td>\n",
       "      <td>18484</td>\n",
       "      <td>0.701984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>thank</td>\n",
       "      <td>6</td>\n",
       "      <td>72871</td>\n",
       "      <td>16785</td>\n",
       "      <td>0.600126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>florida</td>\n",
       "      <td>6</td>\n",
       "      <td>67722</td>\n",
       "      <td>15593</td>\n",
       "      <td>0.390126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>honor</td>\n",
       "      <td>5</td>\n",
       "      <td>69315</td>\n",
       "      <td>16056</td>\n",
       "      <td>0.596250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>foxandfriends</td>\n",
       "      <td>5</td>\n",
       "      <td>61884</td>\n",
       "      <td>15179</td>\n",
       "      <td>0.360155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>american</td>\n",
       "      <td>5</td>\n",
       "      <td>68451</td>\n",
       "      <td>17185</td>\n",
       "      <td>0.349571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>pearl</td>\n",
       "      <td>5</td>\n",
       "      <td>72299</td>\n",
       "      <td>16403</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>big</td>\n",
       "      <td>5</td>\n",
       "      <td>83575</td>\n",
       "      <td>20038</td>\n",
       "      <td>0.498389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>history</td>\n",
       "      <td>5</td>\n",
       "      <td>82737</td>\n",
       "      <td>20390</td>\n",
       "      <td>0.586250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>jones</td>\n",
       "      <td>5</td>\n",
       "      <td>75109</td>\n",
       "      <td>18755</td>\n",
       "      <td>0.493016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>vote</td>\n",
       "      <td>5</td>\n",
       "      <td>71241</td>\n",
       "      <td>16752</td>\n",
       "      <td>0.514556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>christmas</td>\n",
       "      <td>5</td>\n",
       "      <td>93436</td>\n",
       "      <td>25390</td>\n",
       "      <td>0.601643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>rate</td>\n",
       "      <td>4</td>\n",
       "      <td>61656</td>\n",
       "      <td>17022</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>whitehouse</td>\n",
       "      <td>4</td>\n",
       "      <td>48685</td>\n",
       "      <td>11062</td>\n",
       "      <td>0.440833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>brian</td>\n",
       "      <td>4</td>\n",
       "      <td>93143</td>\n",
       "      <td>24617</td>\n",
       "      <td>0.527825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>you</td>\n",
       "      <td>4</td>\n",
       "      <td>69390</td>\n",
       "      <td>15278</td>\n",
       "      <td>0.512986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>clinton</td>\n",
       "      <td>4</td>\n",
       "      <td>120158</td>\n",
       "      <td>34569</td>\n",
       "      <td>0.308654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Country</td>\n",
       "      <td>4</td>\n",
       "      <td>102657</td>\n",
       "      <td>27396</td>\n",
       "      <td>0.542882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>god</td>\n",
       "      <td>4</td>\n",
       "      <td>94576</td>\n",
       "      <td>26992</td>\n",
       "      <td>0.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>abc</td>\n",
       "      <td>4</td>\n",
       "      <td>110580</td>\n",
       "      <td>31469</td>\n",
       "      <td>0.523785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>economy</td>\n",
       "      <td>4</td>\n",
       "      <td>60006</td>\n",
       "      <td>14320</td>\n",
       "      <td>0.294523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Keywords  Frequency  Avg. Likes  Avg. Retweets  Avg. Sentiment\n",
       "25           great         17       74423          15508        0.632480\n",
       "4          america         15       77764          19465        0.631233\n",
       "2             fake         12       88798          23246        0.609117\n",
       "35       pensacola          9       61735          13868        0.372297\n",
       "209           cuts          9       72724          16913        0.517917\n",
       "208            tax          9       61127          15095        0.470904\n",
       "65             cnn          7      112497          29027        0.537169\n",
       "17            news          7       97927          25942        0.581112\n",
       "11          market          7       96695          21615        0.517571\n",
       "6             bill          6       82313          18484        0.701984\n",
       "52           thank          6       72871          16785        0.600126\n",
       "94         florida          6       67722          15593        0.390126\n",
       "56           honor          5       69315          16056        0.596250\n",
       "169  foxandfriends          5       61884          15179        0.360155\n",
       "153       american          5       68451          17185        0.349571\n",
       "148          pearl          5       72299          16403        0.502500\n",
       "81             big          5       83575          20038        0.498389\n",
       "59         history          5       82737          20390        0.586250\n",
       "86           jones          5       75109          18755        0.493016\n",
       "91            vote          5       71241          16752        0.514556\n",
       "103      christmas          5       93436          25390        0.601643\n",
       "114           rate          4       61656          17022        0.275000\n",
       "139     whitehouse          4       48685          11062        0.440833\n",
       "75           brian          4       93143          24617        0.527825\n",
       "53             you          4       69390          15278        0.512986\n",
       "218        clinton          4      120158          34569        0.308654\n",
       "92         Country          4      102657          27396        0.542882\n",
       "99             god          4       94576          26992        0.706250\n",
       "77             abc          4      110580          31469        0.523785\n",
       "18         economy          4       60006          14320        0.294523"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "username = 'realDonaldTrump' # working example: Donald Trump\n",
    "\n",
    "auth = tweepy.OAuthHandler('1X5fCqPl7yVvYxQjQJwkvavFD', 'NXbTDPP3HxlXOL5dWdCegEP09odLAkxUWlyRvZqXxtAtdX597G')\n",
    "auth.set_access_token('925495606931546112-mn3Hda41LsZhbYAKJtddL7TulRKucuj', 'lvCFqSLv5YvOGzCINH6JZ5cBI1CEkPKrRioBn5Iuec3Tt')\n",
    "api = tweepy.API(auth)\n",
    "    \n",
    "tweets_df = pd.DataFrame({\n",
    "    'Timestamp': (),\n",
    "    'Likes': (),\n",
    "    'Retweets': (),\n",
    "    'Text': (),\n",
    "    'Sentences': (),\n",
    "    'Sentiment_Total': (),\n",
    "    'Keywords': ()\n",
    "})\n",
    "\n",
    "tweets_df = tweets_df[['Timestamp', 'Likes', 'Retweets', 'Text', 'Sentences', 'Sentiment_Total', 'Keywords']]\n",
    "    \n",
    "recent_tweets = api.user_timeline(screen_name = username, count=100, tweet_mode=\"extended\") # analyzing 100 tweets\n",
    "for status in recent_tweets:\n",
    "    test = status.full_text\n",
    "    if test[:2] != 'RT': # removing retweets made by the user\n",
    "        status_data = pd.Series([status.created_at, status.favorite_count, status.retweet_count, status.full_text], \n",
    "                                index=['Timestamp', 'Likes', 'Retweets', 'Text'])\n",
    "    tweets_df = tweets_df.append(status_data, ignore_index = True)\n",
    "    \n",
    "tweets_df = tweets_df.drop_duplicates(subset='Text') # just in case, remove any duplicate tweets\n",
    "tweets_df = tweets_df.astype('object')\n",
    "\n",
    "keywords_dict = {}\n",
    "\n",
    "for i in range(len(tweets_df)):\n",
    "    content = tweets_df.iloc[i]['Text']\n",
    "    if 'http' in content:\n",
    "        j = content.index('http')\n",
    "        content = content[:j] # cleaning text of the tweet by removing the link at the end and newline characters\n",
    "    content = content.replace('\\n', '')\n",
    "    tweets_df.iloc[i]['Text'] = content\n",
    "    \n",
    "    blob = TextBlob(content)\n",
    "    tweets_df.iloc[i]['Sentiment_Total'] = blob.sentiment.subjectivity\n",
    "    sentiments = {}\n",
    "    \n",
    "    for sent in blob.sentences: # generating sentiment polarity values for each sentence in the tweet\n",
    "        sentiments[str(sent)] = sent.sentiment.subjectivity\n",
    "        \n",
    "    tweets_df.iloc[i]['Sentences'] = sentiments # insert dictionary of sentence: sentiment value into dataframe\n",
    "    \n",
    "    tweets_df.iloc[i]['Timestamp'] = tweets_df.iloc[i]['Timestamp'].to_pydatetime() # convert pandas.tslib.Timestamp object to datetime\n",
    "    \n",
    "    # Keyword extraction goes here\n",
    "    filtered_words = blob.noun_phrases\n",
    "#     print(filtered_words)\n",
    "    temp = []\n",
    "    \n",
    "    for element in filtered_words:\n",
    "        for x in range(len(filtered_words)):\n",
    "#             print(filtered_words[x])\n",
    "#             print(element)\n",
    "            if element != filtered_words[x] and element in filtered_words[x]:\n",
    "                temp.append(element)\n",
    "                #filtered_words = [x for x in filtered_words if x != element]\n",
    "    parts_of_speech = blob.tags\n",
    "    for element in temp:\n",
    "        filtered_words = [x for x in filtered_words if x != element]\n",
    "    \n",
    "    for x in range(len(parts_of_speech)):\n",
    "        if (parts_of_speech[x])[1] == 'NN':\n",
    "            enter = True\n",
    "            for element in filtered_words:\n",
    "                if (parts_of_speech[x])[0] in element:\n",
    "                    enter = False\n",
    "            if enter:\n",
    "                if x > 0 and (parts_of_speech[x - 1])[1] == 'PRP$':\n",
    "                    filtered_words.append((parts_of_speech[x])[0])\n",
    "    parenthesis = []\n",
    "    paren_init = 0\n",
    "    loc_begin = blob.find(\"(\", paren_init)\n",
    "    loc_end = blob.find(\")\", paren_init)\n",
    "    \n",
    "    while loc_end >= 0:\n",
    "        parenthesis.append(blob[loc_begin:loc_end])\n",
    "        paren_init = loc_end + 1\n",
    "        loc_begin = blob.find(\"(\", paren_init)\n",
    "        loc_end = blob.find(\")\", paren_init)\n",
    "    #print(parenthesis)\n",
    "    \n",
    "    for element in filtered_words:\n",
    "        for pelement in parenthesis:\n",
    "            if element in pelement.lower():\n",
    "                filtered_words = [x for x in filtered_words if x != element]\n",
    "#     tweets_df.iloc[i]['Keywords'] = filtered_words\n",
    "\n",
    "    for word in filtered_words:\n",
    "        separated = TextBlob(word).words\n",
    "        for j in separated:\n",
    "            j = j.strip()\n",
    "            if j.isalpha() and len(j) > 2:\n",
    "                if j in keywords_dict:\n",
    "                    keywords_dict[j][0] += 1\n",
    "                    keywords_dict[j][1] += tweets_df.iloc[i]['Likes']\n",
    "                    keywords_dict[j][2] += tweets_df.iloc[i]['Retweets']\n",
    "                    keywords_dict[j][3] += tweets_df.iloc[i]['Sentiment_Total']\n",
    "#                     sentiment_sum = 0\n",
    "#                     for sent in tweets_df.iloc[i]['Sentences']:\n",
    "#                         if j in sent:\n",
    "#                             sentiment_sum += tweets_df.iloc[i]['Sentences'][sent]\n",
    "#                     keywords_dict[j][3] += sentiment_sum\n",
    "                else:\n",
    "                    keywords_dict[j] = [1, tweets_df.iloc[i]['Likes'], tweets_df.iloc[i]['Retweets'], tweets_df.iloc[i]['Sentiment_Total']]\n",
    "#                     sentiment_sum = 0\n",
    "#                     for sent in tweets_df.iloc[i]['Sentences']:\n",
    "#                         if j in sent:\n",
    "#                             sentiment_sum += tweets_df.iloc[i]['Sentences'][sent]\n",
    "#                     keywords_dict[j][3] = sentiment_sum\n",
    "\n",
    "for key in keywords_dict:\n",
    "    keywords_dict[key][1] = int(keywords_dict[key][1] / keywords_dict[key][0])\n",
    "    keywords_dict[key][2] = int(keywords_dict[key][2] / keywords_dict[key][0])\n",
    "    keywords_dict[key][3] = keywords_dict[key][3] / keywords_dict[key][0]\n",
    "\n",
    "keywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index')\n",
    "keywords_df.columns = ['Frequency', 'Avg. Likes', 'Avg. Retweets', 'Avg. Sentiment']\n",
    "keywords_df.index.name = 'Keywords'\n",
    "keywords_df.reset_index(inplace = True)\n",
    "keywords_df = keywords_df.sort_values(['Frequency'], ascending = [False], na_position = 'last')\n",
    "keywords_df = keywords_df[:30]\n",
    "\n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Proposed method for keyword extraction:\n",
    "\n",
    "1. Tokenize each word with part of speech tag. keep only proper nouns, nouns, adjectives, and verbs.\n",
    "2. Score the nouns and proper nouns based on amount of surrounding adjectives and verbs (using more description tends to indicate importance).\n",
    "3. Record frequency of each word; only keep words that occur above a certain number of times (frequency threshold). These will be our \"keywords\".\n",
    "4. Put the list of keywords for each tweet into the 'Keywords' column of the dataframe.\n",
    "\n",
    "Ideas for graphing the keywords/frequency/likes/retweets relationships:\n",
    "\n",
    "1. y-axis: frequency, x-axis: keyword; simple bar graph of the top keywords\n",
    "\n",
    "2. y-axis: likes/retweet count, x-axis: frequencies of keywords; scatter plot with each dot representing a keyword.\n",
    "\n",
    "3. Simple pie chart to analyze the main content areas that said Twitter account comments on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written below is the function/method oriented version of the program. Different parts of the program are organized into multiple functions. -Isaac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Avg. Likes</th>\n",
       "      <th>Avg. Retweets</th>\n",
       "      <th>Avg. Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>great</td>\n",
       "      <td>17</td>\n",
       "      <td>74435</td>\n",
       "      <td>15510</td>\n",
       "      <td>0.632480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>america</td>\n",
       "      <td>15</td>\n",
       "      <td>77809</td>\n",
       "      <td>19476</td>\n",
       "      <td>0.631233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>12</td>\n",
       "      <td>88893</td>\n",
       "      <td>23269</td>\n",
       "      <td>0.609117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pensacola</td>\n",
       "      <td>9</td>\n",
       "      <td>61756</td>\n",
       "      <td>13872</td>\n",
       "      <td>0.372297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>cuts</td>\n",
       "      <td>9</td>\n",
       "      <td>72725</td>\n",
       "      <td>16914</td>\n",
       "      <td>0.517917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tax</td>\n",
       "      <td>9</td>\n",
       "      <td>61129</td>\n",
       "      <td>15095</td>\n",
       "      <td>0.470904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>cnn</td>\n",
       "      <td>7</td>\n",
       "      <td>112539</td>\n",
       "      <td>29035</td>\n",
       "      <td>0.537169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>news</td>\n",
       "      <td>7</td>\n",
       "      <td>98008</td>\n",
       "      <td>25961</td>\n",
       "      <td>0.581112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>market</td>\n",
       "      <td>7</td>\n",
       "      <td>96742</td>\n",
       "      <td>21626</td>\n",
       "      <td>0.517571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bill</td>\n",
       "      <td>6</td>\n",
       "      <td>82356</td>\n",
       "      <td>18490</td>\n",
       "      <td>0.701984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>thank</td>\n",
       "      <td>6</td>\n",
       "      <td>72886</td>\n",
       "      <td>16788</td>\n",
       "      <td>0.600126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>florida</td>\n",
       "      <td>6</td>\n",
       "      <td>67734</td>\n",
       "      <td>15595</td>\n",
       "      <td>0.390126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>honor</td>\n",
       "      <td>5</td>\n",
       "      <td>69326</td>\n",
       "      <td>16058</td>\n",
       "      <td>0.596250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>foxandfriends</td>\n",
       "      <td>5</td>\n",
       "      <td>61885</td>\n",
       "      <td>15179</td>\n",
       "      <td>0.360155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>american</td>\n",
       "      <td>5</td>\n",
       "      <td>68454</td>\n",
       "      <td>17185</td>\n",
       "      <td>0.349571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>pearl</td>\n",
       "      <td>5</td>\n",
       "      <td>72304</td>\n",
       "      <td>16404</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>big</td>\n",
       "      <td>5</td>\n",
       "      <td>83582</td>\n",
       "      <td>20039</td>\n",
       "      <td>0.498389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>history</td>\n",
       "      <td>5</td>\n",
       "      <td>82747</td>\n",
       "      <td>20392</td>\n",
       "      <td>0.586250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>jones</td>\n",
       "      <td>5</td>\n",
       "      <td>75122</td>\n",
       "      <td>18757</td>\n",
       "      <td>0.493016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>vote</td>\n",
       "      <td>5</td>\n",
       "      <td>71249</td>\n",
       "      <td>16753</td>\n",
       "      <td>0.514556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>christmas</td>\n",
       "      <td>5</td>\n",
       "      <td>93442</td>\n",
       "      <td>25392</td>\n",
       "      <td>0.601643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>rate</td>\n",
       "      <td>4</td>\n",
       "      <td>61664</td>\n",
       "      <td>17023</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>whitehouse</td>\n",
       "      <td>4</td>\n",
       "      <td>48689</td>\n",
       "      <td>11063</td>\n",
       "      <td>0.440833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>brian</td>\n",
       "      <td>4</td>\n",
       "      <td>93153</td>\n",
       "      <td>24620</td>\n",
       "      <td>0.527825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>you</td>\n",
       "      <td>4</td>\n",
       "      <td>69405</td>\n",
       "      <td>15281</td>\n",
       "      <td>0.512986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>clinton</td>\n",
       "      <td>4</td>\n",
       "      <td>120158</td>\n",
       "      <td>34570</td>\n",
       "      <td>0.308654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Country</td>\n",
       "      <td>4</td>\n",
       "      <td>102664</td>\n",
       "      <td>27397</td>\n",
       "      <td>0.542882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>god</td>\n",
       "      <td>4</td>\n",
       "      <td>94585</td>\n",
       "      <td>26994</td>\n",
       "      <td>0.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>abc</td>\n",
       "      <td>4</td>\n",
       "      <td>110589</td>\n",
       "      <td>31471</td>\n",
       "      <td>0.523785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>economy</td>\n",
       "      <td>4</td>\n",
       "      <td>60087</td>\n",
       "      <td>14339</td>\n",
       "      <td>0.294523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Keywords  Frequency  Avg. Likes  Avg. Retweets  Avg. Sentiment\n",
       "25           great         17       74435          15510        0.632480\n",
       "4          america         15       77809          19476        0.631233\n",
       "2             fake         12       88893          23269        0.609117\n",
       "35       pensacola          9       61756          13872        0.372297\n",
       "209           cuts          9       72725          16914        0.517917\n",
       "208            tax          9       61129          15095        0.470904\n",
       "65             cnn          7      112539          29035        0.537169\n",
       "17            news          7       98008          25961        0.581112\n",
       "11          market          7       96742          21626        0.517571\n",
       "6             bill          6       82356          18490        0.701984\n",
       "52           thank          6       72886          16788        0.600126\n",
       "94         florida          6       67734          15595        0.390126\n",
       "56           honor          5       69326          16058        0.596250\n",
       "169  foxandfriends          5       61885          15179        0.360155\n",
       "153       american          5       68454          17185        0.349571\n",
       "148          pearl          5       72304          16404        0.502500\n",
       "81             big          5       83582          20039        0.498389\n",
       "59         history          5       82747          20392        0.586250\n",
       "86           jones          5       75122          18757        0.493016\n",
       "91            vote          5       71249          16753        0.514556\n",
       "103      christmas          5       93442          25392        0.601643\n",
       "114           rate          4       61664          17023        0.275000\n",
       "139     whitehouse          4       48689          11063        0.440833\n",
       "75           brian          4       93153          24620        0.527825\n",
       "53             you          4       69405          15281        0.512986\n",
       "218        clinton          4      120158          34570        0.308654\n",
       "92         Country          4      102664          27397        0.542882\n",
       "99             god          4       94585          26994        0.706250\n",
       "77             abc          4      110589          31471        0.523785\n",
       "18         economy          4       60087          14339        0.294523"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "username = 'realDonaldTrump' # working example: Donald Trump\n",
    "auth = tweepy.OAuthHandler('1X5fCqPl7yVvYxQjQJwkvavFD', 'NXbTDPP3HxlXOL5dWdCegEP09odLAkxUWlyRvZqXxtAtdX597G')\n",
    "auth.set_access_token('925495606931546112-mn3Hda41LsZhbYAKJtddL7TulRKucuj', 'lvCFqSLv5YvOGzCINH6JZ5cBI1CEkPKrRioBn5Iuec3Tt')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def tweet_collector(username): \n",
    "    tweets_df = pd.DataFrame({\n",
    "        'Timestamp': (),\n",
    "        'Likes': (),\n",
    "        'Retweets': (),\n",
    "        'Text': (),\n",
    "        'Sentences': (),\n",
    "        'Sentiment_Total': (),\n",
    "        'Keywords': ()\n",
    "    })\n",
    "\n",
    "    tweets_df = tweets_df[['Timestamp', 'Likes', 'Retweets', 'Text', 'Sentences', 'Sentiment_Total', 'Keywords']]\n",
    "\n",
    "    recent_tweets = api.user_timeline(screen_name = username, count=100, tweet_mode=\"extended\") # analyzing 100 tweets\n",
    "    for status in recent_tweets:\n",
    "        test = status.full_text\n",
    "        if test[:2] != 'RT': # removing retweets made by the user\n",
    "            status_data = pd.Series([status.created_at, status.favorite_count, status.retweet_count, status.full_text], \n",
    "                                    index=['Timestamp', 'Likes', 'Retweets', 'Text'])\n",
    "        tweets_df = tweets_df.append(status_data, ignore_index = True)\n",
    "\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='Text') # just in case, remove any duplicate tweets\n",
    "    tweets_df = tweets_df.astype('object')\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "def keyword_data(tweets_df):\n",
    "    keywords_dict = {}\n",
    "\n",
    "    for i in range(len(tweets_df)):\n",
    "        content = tweets_df.iloc[i]['Text']\n",
    "        if 'http' in content:\n",
    "            j = content.index('http')\n",
    "            content = content[:j] # cleaning text of the tweet by removing the link at the end and newline characters\n",
    "        content = content.replace('\\n', '')\n",
    "        tweets_df.iloc[i]['Text'] = content\n",
    "\n",
    "        blob = TextBlob(content)\n",
    "        tweets_df.iloc[i]['Sentiment_Total'] = blob.sentiment.subjectivity\n",
    "        sentiments = {}\n",
    "\n",
    "        for sent in blob.sentences: # generating sentiment polarity values for each sentence in the tweet\n",
    "            sentiments[str(sent)] = sent.sentiment.subjectivity\n",
    "\n",
    "        tweets_df.iloc[i]['Sentences'] = sentiments # insert dictionary of sentence: sentiment value into dataframe\n",
    "\n",
    "        tweets_df.iloc[i]['Timestamp'] = tweets_df.iloc[i]['Timestamp'].to_pydatetime() # convert pandas.tslib.Timestamp object to datetime\n",
    "\n",
    "        # Keyword extraction goes here\n",
    "        filtered_words = blob.noun_phrases\n",
    "    #     print(filtered_words)\n",
    "        temp = []\n",
    "\n",
    "        for element in filtered_words:\n",
    "            for x in range(len(filtered_words)):\n",
    "    #             print(filtered_words[x])\n",
    "    #             print(element)\n",
    "                if element != filtered_words[x] and element in filtered_words[x]:\n",
    "                    temp.append(element)\n",
    "                    #filtered_words = [x for x in filtered_words if x != element]\n",
    "        parts_of_speech = blob.tags\n",
    "        for element in temp:\n",
    "            filtered_words = [x for x in filtered_words if x != element]\n",
    "\n",
    "        for x in range(len(parts_of_speech)):\n",
    "            if (parts_of_speech[x])[1] == 'NN':\n",
    "                enter = True\n",
    "                for element in filtered_words:\n",
    "                    if (parts_of_speech[x])[0] in element:\n",
    "                        enter = False\n",
    "                if enter:\n",
    "                    if x > 0 and (parts_of_speech[x - 1])[1] == 'PRP$':\n",
    "                        filtered_words.append((parts_of_speech[x])[0])\n",
    "        parenthesis = []\n",
    "        paren_init = 0\n",
    "        loc_begin = blob.find(\"(\", paren_init)\n",
    "        loc_end = blob.find(\")\", paren_init)\n",
    "\n",
    "        while loc_end >= 0:\n",
    "            parenthesis.append(blob[loc_begin:loc_end])\n",
    "            paren_init = loc_end + 1\n",
    "            loc_begin = blob.find(\"(\", paren_init)\n",
    "            loc_end = blob.find(\")\", paren_init)\n",
    "        #print(parenthesis)\n",
    "\n",
    "        for element in filtered_words:\n",
    "            for pelement in parenthesis:\n",
    "                if element in pelement.lower():\n",
    "                    filtered_words = [x for x in filtered_words if x != element]\n",
    "    #     tweets_df.iloc[i]['Keywords'] = filtered_words\n",
    "\n",
    "        for word in filtered_words:\n",
    "            separated = TextBlob(word).words\n",
    "            for j in separated:\n",
    "                j = j.strip()\n",
    "                if j.isalpha() and len(j) > 2:\n",
    "                    if j in keywords_dict:\n",
    "                        keywords_dict[j][0] += 1\n",
    "                        keywords_dict[j][1] += tweets_df.iloc[i]['Likes']\n",
    "                        keywords_dict[j][2] += tweets_df.iloc[i]['Retweets']\n",
    "                        keywords_dict[j][3] += tweets_df.iloc[i]['Sentiment_Total']\n",
    "                    else:\n",
    "                        keywords_dict[j] = [1, tweets_df.iloc[i]['Likes'], tweets_df.iloc[i]['Retweets'], tweets_df.iloc[i]['Sentiment_Total']]\n",
    "\n",
    "    for key in keywords_dict:\n",
    "        keywords_dict[key][1] = int(keywords_dict[key][1] / keywords_dict[key][0])\n",
    "        keywords_dict[key][2] = int(keywords_dict[key][2] / keywords_dict[key][0])\n",
    "        keywords_dict[key][3] = keywords_dict[key][3] / keywords_dict[key][0]\n",
    "        \n",
    "    return keywords_dict\n",
    "\n",
    "def to_dataframe(keywords_dict):\n",
    "    keywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index')\n",
    "    keywords_df.columns = ['Frequency', 'Avg. Likes', 'Avg. Retweets', 'Avg. Sentiment']\n",
    "    keywords_df.index.name = 'Keywords'\n",
    "    keywords_df.reset_index(inplace = True)\n",
    "    keywords_df = keywords_df.sort_values(['Frequency'], ascending = [False], na_position = 'last')\n",
    "    keywords_df = keywords_df[:30]\n",
    "    \n",
    "    return keywords_df\n",
    "\n",
    "data_df = tweet_collector(username)\n",
    "keywords = keyword_data(data_df)\n",
    "key_df = to_dataframe(keywords)\n",
    "\n",
    "key_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
